from __future__ import division
from __future__ import print_function
from __future__ import absolute_import

# Suppresses warnings
# Warnings were noted prior to cleaning up the notebook, suppressing them are for aesthetics
import warnings
warnings.filterwarnings(action='ignore')

import time
import copy
import random
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from timeit import default_timer as timer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.metrics import confusion_matrix
from imblearn.metrics import classification_report_imbalanced
from imblearn import pipeline
from imblearn.combine import SMOTETomek
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import TomekLinks

# Initialise seed value - 42
seed = 42
random.seed(42)
tf.set_random_seed(seed)
np.random.seed(seed)
sns.set_style("whitegrid")

get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")



#minority class data for training GANs[X_train,y_train]

train_df = pd.concat([X_train,y_train], join='outer', axis='columns')
subsetDataFrame_X = train_df[train_df['class'] == 1]
print(subsetDataFrame_X.shape)


FTrain_X1 = subsetDataFrame_X.drop(labels=['class'], axis='columns')
FTrain_Y1 = subsetDataFrame_X['class']



#BEGAN

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import os


mb_size = 5000
X_dim = 400
z_dim = 100
h_dim = 128
lr = 1e-3
m = 5
lam = 1e-2
gamma = 0.5
k_curr = 0
gan_epochs = 2000
gan_Default_X = {}
gan_Default_loss = []

X = tf.placeholder(tf.float32, shape=[None, X_dim])
z = tf.placeholder(tf.float32, shape=[None, z_dim])
k = tf.placeholder(tf.float32)

D_W1 = tf.Variable(xavier_init([X_dim, h_dim]))
D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))
D_W2 = tf.Variable(xavier_init([h_dim, X_dim]))
D_b2 = tf.Variable(tf.zeros(shape=[X_dim]))

G_W1 = tf.Variable(xavier_init([z_dim, h_dim]))
G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))
G_W2 = tf.Variable(xavier_init([h_dim, X_dim]))
G_b2 = tf.Variable(tf.zeros(shape=[X_dim]))

theta_G = [G_W1, G_W2, G_b1, G_b2]
theta_D = [D_W1, D_W2, D_b1, D_b2]


def sample_z(m, n):
    return np.random.uniform(-1., 1., size=[m, n])

def steady_epoch(loss_lst, num_epochs_ran, sd_fluc_pct=0.15, scan_frame=225, stab_fn=min):   
    """ 
    FOR THE PURPOSE OF FINDING A FRAME OF EPOCHS WHERE THE LOSSES HAS STABILISED WITHIN SPECIFIED PERCENTAGE OF 
    1SD OF LOSSES FOR ALL EPOCHS EXECUTED
    
    DEFAULT FRAME OF EPOCHS WHERE LOSS IS CONSIDERED STEADY IS 225, FLUCTUATING WITHIN +/- 15% OF 1 S.D. OF THE LOSSES
    
    [EXAMPLE - 5% OF S.D., 5% OF TOTAL EPOCHS RAN]
    Obtain minimum of losses and its corresponding 1 s.d. of loss of the given array of losses
    Do for a list of losses
        For some 5% out of all epochs ran
            if that particular 5% of epochs have losses each within +/-5% of the minimum loss,
                Epoch frame chosen as steady epoch-period
                Else return 'no steady epoch-periods found'; break
        Return frame of epochs found 
    """
    # Obtain minimum and s.d. value of the losses for all epochs
    loss_1sd = np.std(loss_lst)
    stab_fn_loss = stab_fn(loss_lst)

    epoch_frame_start = 0
    epoch_frame_end = epoch_frame_start + scan_frame
    
    # Return nothing in the event no frame of losses are found to have steady losses
    steady_frame=None
    
    # For loop termination
    exit_loop = True
    while exit_loop:

        # Reset counter when one of the epoch is not within +/-'sd_fluc_pct'% of minimum losses
        counter_5pct = 0

        for epoch in range(epoch_frame_start, epoch_frame_end, 1):
            if (loss_lst[epoch] > stab_fn_loss and loss_lst[epoch] <= (stab_fn_loss + sd_fluc_pct*loss_1sd))             or (loss_lst[epoch] < stab_fn_loss and loss_lst[epoch] >= (stab_fn_loss - sd_fluc_pct*loss_1sd)):
                
                # Increase counter progressively until all (225) epochs in 'scan_frame' are within specified tolerance
                counter_5pct += 1

                # When (225) epochs are within +/-'sd_fluc_pct'% of minimal loss, flag the (225th) final epoch
                if counter_5pct == scan_frame:
                    print('Steady epoch frame found at epoch {} as final'.format(epoch_frame_end))
                    
                    # Return numpy array of epoch numbers within (225) specified range
                    steady_frame = np.linspace(start=(epoch_frame_end-scan_frame+1), stop=epoch_frame_end, num=scan_frame)
                    exit_loop = False
                    break           
            else:
                break

        # When a frame of 225 epochs within +/-'sd_fluc_pct'% of minimum losses is not found,
            # shift to next frame by 1 epoch
        epoch_frame_start += 1
        epoch_frame_end += 1

        if epoch_frame_end > num_epochs_ran:
            print('No steady epoch frame found!')
            break

    return steady_frame



def G(z):
    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)
    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2
    G_prob = tf.nn.sigmoid(G_log_prob)
    return G_prob


def D(X):
    D_h1 = tf.nn.relu(tf.matmul(X, D_W1) + D_b1)
    X_recon = tf.matmul(D_h1, D_W2) + D_b2
    return tf.reduce_mean(tf.reduce_sum((X - X_recon)**2, 1))


G_sample = G(z)
D_real = D(X)
D_fake = D(G_sample)


D_loss = D_real - k*D_fake
G_loss = D_fake


D_solver = (tf.train.AdamOptimizer(learning_rate=lr).minimize(D_loss, var_list=theta_D))

G_solver = (tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss, var_list=theta_G))


sess = tf.Session()
sess.run(tf.global_variables_initializer())
# Measure Start Time
start_time = time.time()
if not os.path.exists('out/'):
    os.makedirs('out/')
n_sample =4388
i = 0
Xtr, Ytr1 = next_batch(4388, FTrain_X1,FTrain_Y1)
Ytr = pd.DataFrame(Ytr1, columns=y_cols)
for epoch in range(gan_epochs):
    

    _, D_real_curr = sess.run([D_solver, D_real],feed_dict={X: Xtr, z: sample_z(mb_size, z_dim), k: k_curr})

    _, D_fake_curr = sess.run([G_solver, D_fake],feed_dict={X: Xtr, z: sample_z(mb_size, z_dim)})

    k_curr = k_curr + lam * (gamma*D_real_curr - D_fake_curr)
    
    # Store losses per epoch
    gan_Default_loss.append((D_real_curr, D_fake_curr))
                            
    if (epoch+1) % 1000 == 0 or (epoch+1) == 1:
                            
        print('Iter: {}'.format(epoch+1))
        print('Fraud generator loss: {} | discriminator loss: {}'.format(D_fake_curr, D_real_curr), '\n')
         #print('D loss: {:.4}'. format(D_loss_curr))
         #print('G_loss: {:.4}'.format(G_loss_curr))
        
    #Z_sample_gen = sess.run(G_sample, feed_dict={z: sample_z(n_sample, z_dim)})    
   # Z_sample_gen = sample_Z(n_sample, Z_dim)
    gan_Default_X[epoch+1] = sess.run(G_sample, feed_dict={z: sample_z(n_sample, z_dim)})
    
    # Measure End Time
elapsed_time = time.time() - start_time
print('Time elapsed to train: ', elapsed_time)

   




    



# Plot loss results
epoch_axis = np.linspace(1, gan_epochs, num=gan_epochs)

# Extract generator and discriminator losses from 'gan_fraud' object
gen_losses_Default = [gen[1] for gen in gan_Default_loss]
disc_losses_Default = [disc[0] for disc in gan_Default_loss]

# Plot discriminator/generator losses of fraud/fraudless beside each other
_ = plt.figure(figsize=(16,7))

_ = plt.subplot(1, 2, 2)
_ = plt.plot(epoch_axis, gen_losses_Default, label='Generator')
_ = plt.plot(epoch_axis, disc_losses_Default, label='Discriminator')
_ = plt.xlabel('Epochs'); _ = plt.ylabel('Losses')
_ = plt.legend(loc='best'); _ = plt.title('Classic Fraud Generator/Discriminator Losses Over Epochs')
_ = plt.tight_layout(); plt.margins(0.02)

plt.show()

#######

# Obtain steady epoch for the fraudulent GAN
    # Stabilise within +/-5% of total epochs ran, all epochs within this frame must have losses fluctuating about 0.75 s.d.
dloss_sdy_Default = steady_epoch(disc_losses_Default, num_epochs_ran=gan_epochs, sd_fluc_pct=0.75, 
                               scan_frame=int(gan_epochs*0.005), stab_fn=np.median)
gloss_sdy_Default = steady_epoch(gen_losses_Default, num_epochs_ran=gan_epochs, sd_fluc_pct=0.75, 
                               scan_frame=int(gan_epochs*0.005), stab_fn=np.median)
Default_sdy_epoch = int(max(max(dloss_sdy_Default), max(gloss_sdy_Default)))

print('Default steady epoch: {}'.format(Default_sdy_epoch))





