from __future__ import division
from __future__ import print_function
from __future__ import absolute_import

# Suppresses warnings
# Warnings were noted prior to cleaning up the notebook, suppressing them are for aesthetics
import warnings
warnings.filterwarnings(action='ignore')

import time
import copy
import random
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from timeit import default_timer as timer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.metrics import confusion_matrix
from imblearn.metrics import classification_report_imbalanced
from imblearn import pipeline
from imblearn.combine import SMOTETomek
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import TomekLinks

# Initialise seed value - 42
seed = 42
random.seed(42)
tf.set_random_seed(seed)
np.random.seed(seed)
sns.set_style("whitegrid")

get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")



#minority class data for training GANs[X_train,y_train]

train_df = pd.concat([X_train,y_train], join='outer', axis='columns')
subsetDataFrame_X = train_df[train_df['class'] == 1]
print(subsetDataFrame_X.shape)


FTrain_X1 = subsetDataFrame_X.drop(labels=['class'], axis='columns')
FTrain_Y1 = subsetDataFrame_X['class']


#WGAN 

gan_epochs = 2000  #Training epochs
mb_size = 5000     #sample size
Z_dim = 100    #z noise
X_dim = 400   #X data dimension
y_dim = 1     #Y data dimension
h_dim = 500
# Collect generated fraud data; Epoch as key, array as values
gan_Default_X = {}

# Measure Start Time
start_time = time.time()
# Collect list of generator/discriminator losses
gan_Default_loss = []


# Discriminator Net
X = tf.placeholder(tf.float32, shape=[None, X_dim], name='X')

D_W1 = tf.Variable(xavier_init([X_dim, 128]), name='D_W1')
D_b1 = tf.Variable(tf.zeros(shape=[128]), name='D_b1')

D_W2 = tf.Variable(xavier_init([128, 1]), name='D_W2')
D_b2 = tf.Variable(tf.zeros(shape=[1]), name='D_b2')

theta_D = [D_W1, D_W2, D_b1, D_b2]

# Generator Net
Z = tf.placeholder(tf.float32, shape=[None, 100], name='Z')

G_W1 = tf.Variable(xavier_init([100, 128]), name='G_W1')
G_b1 = tf.Variable(tf.zeros(shape=[128]), name='G_b1')

G_W2 = tf.Variable(xavier_init([128, X_dim]), name='G_W2')
G_b2 = tf.Variable(tf.zeros(shape=[X_dim]), name='G_b2')

theta_G = [G_W1, G_W2, G_b1, G_b2]


def generator(z):
    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)
    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2
    G_prob = tf.nn.sigmoid(G_log_prob)

    return G_prob


def discriminator(x):
    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)
    out = tf.matmul(D_h1, D_W2) + D_b2
    
    return out

def sample_Z(m, n):
    '''Uniform prior for G(Z)'''
    return np.random.uniform(-1., 1., size=[m, n])


G_sample = generator(Z)
D_real = discriminator(X)
D_fake = discriminator(G_sample)

D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)
G_loss = -tf.reduce_mean(D_fake)
#Then we train the networks one by one with those Adversarial Training, represented by those loss functions above.

# Only update D(X)'s parameters, so var_list = theta_D
D_solver = tf.train.GradientDescentOptimizer(learning_rate=0.000001).minimize(D_loss, var_list=theta_D)
# Only update G(X)'s parameters, so var_list = theta_G
G_solver = tf.train.GradientDescentOptimizer(learning_rate=0.000001).minimize(G_loss, var_list=theta_G)
clip_D = [p.assign(tf.clip_by_value(p, -0.001, 0.001)) for p in theta_D]
sess = tf.Session()
sess.run(tf.global_variables_initializer())

 

if not os.path.exists('out/'):
    os.makedirs('out/')

i = 0
n_sample = 5000
for epoch in range(gan_epochs):
    
    Z_sample = sample_Z(mb_size, Z_dim)
    Xtr, Ytr1 = next_batch(5000, X_train_df,y_train_df)
    Ytr = pd.DataFrame(Ytr1, columns=y_cols)
    
    _, D_loss_curr,_ = sess.run([D_solver, D_loss,clip_D], feed_dict={X: Xtr, Z: sample_Z(mb_size, Z_dim)})
    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})
    
    gan_Default_loss.append((D_loss_curr, G_loss_curr))
                            
    if (epoch+1) % 1000 == 0 or (epoch+1) == 1:
                            
        print('Iter: {}'.format(epoch+1))
        print('Fraud generator loss: {} | discriminator loss: {}'.format(G_loss_curr, D_loss_curr), '\n')
         
        
    Z_sample_gen = sample_Z(n_sample, Z_dim)
    gan_Default_X[epoch+1] =sess.run(G_sample, feed_dict={Z: Z_sample})
    
    # Measure End Time
elapsed_time = time.time() - start_time
print('Time elapsed to train: ', elapsed_time)
  





# Plot loss results
epoch_axis = np.linspace(1, gan_epochs, num=gan_epochs)

# Extract generator and discriminator losses from 'gan_fraud' object
gen_losses_Default = [gen[1] for gen in gan_Default_loss]
disc_losses_Default = [disc[0] for disc in gan_Default_loss]

# Plot discriminator/generator losses of fraud/fraudless beside each other
_ = plt.figure(figsize=(16,7))

_ = plt.subplot(1, 2, 2)
_ = plt.plot(epoch_axis, gen_losses_Default, label='Generator')
_ = plt.plot(epoch_axis, disc_losses_Default, label='Discriminator')
_ = plt.xlabel('Epochs'); _ = plt.ylabel('Losses')
_ = plt.legend(loc='best'); _ = plt.title('Classic Fraud Generator/Discriminator Losses Over Epochs')
_ = plt.tight_layout(); plt.margins(0.02)

plt.show()

#######

# Obtain steady epoch for the fraudulent GAN
    # Stabilise within +/-5% of total epochs ran, all epochs within this frame must have losses fluctuating about 0.75 s.d.
dloss_sdy_Default = steady_epoch(disc_losses_Default, num_epochs_ran=gan_epochs, sd_fluc_pct=0.75, 
                               scan_frame=int(gan_epochs*0.005), stab_fn=np.median)
gloss_sdy_Default = steady_epoch(gen_losses_Default, num_epochs_ran=gan_epochs, sd_fluc_pct=0.75, 
                               scan_frame=int(gan_epochs*0.005), stab_fn=np.median)
Default_sdy_epoch = int(max(max(dloss_sdy_Default), max(gloss_sdy_Default)))

print('Default steady epoch: {}'.format(Default_sdy_epoch))









